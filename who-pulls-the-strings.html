<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Who Pulls the Strings? Algorithmic Bias and the Erosion of Individual Sovereignty - Isabella Calmet</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <header>
        <h1>Isabella Calmet's Blog</h1>
        <nav>
            <a href="index.html">Home</a> |
            <a href="about.html">About</a>
        </nav>
    </header>

    <main>
        <article>
            <h2>Who Pulls the Strings? Algorithmic Bias and the Erosion of Individual Sovereignty</h2>
            <p class="date">February 16, 2026</p>

            <p>In my previous post, we explored the unsettling possibility that Artificial Intelligence knows us better than we know ourselves. But today, I want to dive into a darker layer of this reality: <strong>Algorithmic Bias</strong>.</p>

            <p>If we accept that AI has the capacity to predict our desires, we must ask: what happens when that same tool begins to shape, almost imperceptibly, our perception of the world? If algorithms learn to manipulate our inclinations, who is truly the one making the decisions?</p>

            <h3>The Trojan Horse in the Code</h3>
            <p>We often think of AI as a neutral refereeâ€”a giant calculator delivering the "best" option. However, the <a href="https://www.nnlm.gov/guides/data-thesaurus/algorithmic-bias">National Network of Libraries of Medicine</a> defines algorithmic bias as systematic and repeatable errors in a computer system that create unfair outcomes (NNLM).</p>

            <p>These biases are not accidents; they are reflections of the data the machine is trained on or the priorities of those who programmed it. When you enter a social network or consult a recommendation system, you aren't seeing reality; you are seeing a version of reality curated to keep you engaged.</p>

            <h3>The Strategic Advantage: Manipulating Emotions and Patterns</h3>
            <p>Modern AI has a tactical advantage over the human being: it doesn't sleep, it doesn't tire, and it analyzes patterns we don't even notice. Research published in <a href="https://www.science.org/doi/abs/10.1126/science.abp9364">Science</a> has demonstrated how algorithms can amplify racial and socioeconomic biases in health and justice systems (Obermeyer et al.).</p>

            <p>A study in <a href="https://www.sciencedirect.com/science/article/abs/pii/S0020025521012901">Information Sciences</a> describes how deep learning algorithms can create "information cocoons" that limit the user's free will, moving them toward extreme positions without them even noticing (ScienceDirect).</p>

            <h3>The Schism of Liberty: Own Ideas or Implanted Ideas?</h3>
            <p>From a libertarian standpoint, the individual must be the architect of their own destiny. But algorithmic bias acts as an interference in the marketplace of ideas. If the algorithm decides what information reaches your eyes, it is limiting your ability to judge for yourself.</p>

            <h3>Digital Health and Bias in Diagnosis</h3>
            <p>An article in <a href="https://journals.plos.org/digitalhealth/article?id=10.1371/journal.pdig.0000278">PLOS Digital Health</a> warns that AI algorithms in healthcare often fail to represent diverse populations, leading to misdiagnosis (PLOS). If we entrust our health to a system that has a biased view of human biology, we are giving up our right to receive treatment based on objective truth.</p>

            <h3>The 50/50 Rule: Reclaiming Control</h3>
            <p>To survive this algorithmic era, we must apply the <strong>50/50 Rule</strong>: use AI to process the data, but reserve 50% of your judgment to question the source, motive, and bias of the recommendation. Individual sovereignty requires us to be able to say "no" to the algorithm, even if that path is "inefficient."</p>

            <p>As <a href="https://samlevine277.github.io/ai-in-ruby-training.html">Sam Levine</a> concludes in his work, the future is about humans alongside machines, but only if the human remains in command. We must protect our minds from digital domestication.</p>

            <hr>
            <h4>Works Cited</h4>
            <ul>
                <li>National Network of Libraries of Medicine (NNLM). "Algorithmic Bias: Data Thesaurus." 2024.</li>
                <li>Obermeyer, Z., et al. "Dissecting racial bias in an algorithm used to manage the health of populations." <em>Science</em>, 2019.</li>
                <li>PLOS Digital Health. "Addressing algorithmic bias in healthcare AI." 2023.</li>
                <li>ScienceDirect. "Deep learning and the impact of information filtering on human decision-making." <em>Information Sciences</em>, 2022.</li>
            </ul>
        </article>
    </main>

    <footer>
        <p>&copy; 2026 Isabella Calmet</p>
    </footer>
</body>
</html>

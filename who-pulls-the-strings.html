<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Who Pulls the Strings? Algorithmic Bias and the Erosion of Individual Sovereignty - Isabella Calmet</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <header>
        <h1>Isabella Calmet's Blog</h1>
        <nav>
            <a href="index.html">Home</a> |
            <a href="about.html">About</a>
        </nav>
    </header>

    <main>
        <article>
            <h2>Who Pulls the Strings? Algorithmic Bias and the Erosion of Individual Sovereignty</h2>
            <p class="date">February 16, 2026</p>

            <p>In my previous post, we explored the unsettling possibility that Artificial Intelligence knows us better than we know ourselves. But today, I want to dive into a darker layer of this reality: <strong>Algorithmic Bias</strong>. If we accept that AI has the capacity to predict our desires, we must ask: what happens when that same tool begins to shape, almost imperceptibly, our perception of the world? If algorithms learn to manipulate our inclinations, <strong>who is truly the one making the decisions?</strong></p>

            <h3>The Trojan Horse in the Code</h3>
            <p>We often think of AI as a neutral referee—a giant calculator delivering the "best" option. However, the <a href="https://www.nnlm.gov/guides/data-thesaurus/algorithmic-bias">National Network of Libraries of Medicine</a> defines algorithmic bias as systematic and repeatable errors in a computer system that create unfair outcomes (NNLM). These biases are not accidents; they are reflections of the data the machine is trained on or the priorities of those who programmed it.</p>

            <p>When you enter a social network or consult a recommendation system, you aren't seeing reality; you are seeing a version of reality curated to keep you engaged. Here arises the question: if we come to blindly trust AI and it recommends a path, how do we know it is what’s right for us and not simply what the AI "wants" us to do to optimize its own metrics?</p>

            <h3>The Strategic Advantage: Manipulating Emotions and Patterns</h3>
            <p>Modern AI has a tactical advantage over the human being: it doesn't sleep, it doesn't tire, and it analyzes patterns we don't even notice. Research published in <a href="https://www.science.org/doi/abs/10.1126/science.abp9364">Science</a> has demonstrated how algorithms can amplify racial and socioeconomic biases in health and justice systems (Obermeyer et al.).</p>

            <p>A study in <a href="https://www.sciencedirect.com/science/article/abs/pii/S0020025521012901">Information Sciences</a> describes how deep learning algorithms can create "information cocoons" that limit the user's free will, moving them toward extreme positions without them even noticing (ScienceDirect). This is the "Productivity Paradox" in reverse: we think we are choosing more efficiently, but we are actually being steered into a narrower funnel of choices.</p>

            <h3>The Schism of Liberty: Own Ideas or Implanted Ideas?</h3>
            <p>From a libertarian standpoint, the individual must be the architect of their own destiny. But algorithmic bias acts as an interference in the marketplace of ideas. If the algorithm decides what information reaches your eyes, it is limiting your ability to judge for yourself. We are witnessing a <strong>Schism of Liberty</strong> where we cannot distinguish between our genuine desires and the "optimal inclinations" suggested by a server in Silicon Valley.</p>

            <h3>Digital Health and Bias in Diagnosis</h3>
            <p>As I noted in my discussion on the CRISPR revolution, precision is paramount in medicine. However, an article in <a href="https://journals.plos.org/digitalhealth/article?id=10.1371/journal.pdig.0000278">PLOS Digital Health</a> warns that AI algorithms in healthcare often fail to represent diverse populations, leading to misdiagnosis (PLOS). If we entrust our health to a system that has a biased view of human biology, we are giving up our right to receive treatment based on objective truth. This is not just a technical error; it is an erosion of <strong>Bio-Sovereignty</strong>.</p>

            <h3>Conclusion: Who Pulls the Strings?</h3>
            <p>To survive this algorithmic era, we must not be paranoid, but we must be skeptical. The greatest threat to our autonomy doesn't come from big corporations or the State, but from the <strong>vigilance of the individual</strong>. We should use AI as an incredible productivity tool, but only if the human remains in command. To achieve this, we must apply the <strong>50/50 Rule</strong>: use AI to process the data, but reserve 50% of your judgment to question the source, motive, and bias of the recommendation.</p>

            <p>Individual sovereignty requires us to be able to say "no" to the algorithm, to choose the inefficient path, to read the opinion that upsets us, and to protect our minds from digital domestication. At the end of the day, who we are depends on the decisions we make. If we let AI make those decisions for us—or if we let its biases narrow our world—we will have lost the most valuable property we have: our own freedom. The strings are invisible, but that doesn't mean they aren't there. It is time to stop being the puppet and start being the architect again.</p>

            <hr>
            <h4>Works Cited</h4>
            <ul>
                <li>National Network of Libraries of Medicine (NNLM). "Algorithmic Bias: Data Thesaurus." 2024.</li>
                <li>Obermeyer, Z., et al. "Dissecting racial bias in an algorithm used to manage the health of populations." <em>Science</em>, 366(6464), 2019.</li>
                <li>PLOS Digital Health. "Addressing algorithmic bias in healthcare AI." 2023.</li>
                <li>ScienceDirect. "Deep learning and the impact of information filtering on human decision-making." <em>Information Sciences</em>, 2022.</li>
            </ul>
        </article>
    </main>

    <footer>
        <p>&copy; 2026 Isabella Calmet</p>
    </footer>
</body>
</html>
